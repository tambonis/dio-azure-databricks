# Aprendizados do Projeto Azure Databricks

## Este projeto apresenta uma visÃ£o prÃ¡tica do ambiente Azure utilizando uma conta gratuita de estudante. Abaixo estÃ£o os principais tÃ³picos abordados:

ðŸ”¹ CriaÃ§Ã£o de Assinatura Azure: Uso de conta gratuita para ativar e configurar assinatura de estudante no portal Azure.

ðŸ”¹ CriaÃ§Ã£o de Grupos de Recursos: OrganizaÃ§Ã£o lÃ³gica dos recursos do Azure para facilitar gerenciamento e controle.

ðŸ”¹ Boas PrÃ¡ticas de Nomenclatura: AplicaÃ§Ã£o de padrÃµes para nomes de recursos visando padronizaÃ§Ã£o e legibilidade.

ðŸ”¹ ConfiguraÃ§Ã£o do Azure Data Factory: PreparaÃ§Ã£o do ambiente para orquestraÃ§Ã£o de dados e integraÃ§Ã£o com outros serviÃ§os.

ðŸ”¹ Monitoramento de Uso e Custos:

ConfiguraÃ§Ã£o de dashboards personalizados;

Uso de mÃ©tricas de consumo e alertas de custo para controle financeiro.

ðŸ”¹ VisualizaÃ§Ã£o dos Dados de Consumo: InterpretaÃ§Ã£o e anÃ¡lise dos gastos no portal Azure.

ðŸ”¹ Infraestrutura como CÃ³digo:

CriaÃ§Ã£o de templates com ARM Templates;

ImplantaÃ§Ã£o automatizada de recursos.

ðŸ”¹ AutomaÃ§Ã£o com Azure Cloud Shell:

UtilizaÃ§Ã£o de comandos via terminal;

Scripts de automaÃ§Ã£o para criaÃ§Ã£o e gerenciamento de recursos.

# Lab de copiar uma tabela de teste de um SQL Server on-premises self-hosted para o Azure Blob Storage, usando o Azure Data Factory (ADF) por meio de um pipeline, - 30/05/2025

![image](https://github.com/user-attachments/assets/6ca04a7b-6390-4fdc-8270-03522747d636)

Conceitos:

* **Linked Service**
* **Dataset**
* **Pipeline**
* **Self-hosted Integration Runtime**
* **Medallion Architecture** (nÃ­vel Bronze)

## âœ… **Objetivo:**

Mover os dados da tabela `tabela_teste` do SQL Server local para um container no Azure Blob Storage, representando a **camada Bronze** da Medallion Architecture.

## ðŸ§° **Tecnologias envolvidas**

* SQL Server (on-premises)
* Azure Blob Storage
* Azure Data Factory (ADF)
* Self-hosted Integration Runtime (SHIR)

## ðŸ§± Medallion Architecture

| Camada     | DescriÃ§Ã£o                                    |
| ---------- | -------------------------------------------- |
| **Bronze** | Dados brutos, copiados diretamente da origem |
| Silver     | Dados limpos e transformados                 |
| Gold       | Dados prontos para consumo analÃ­tico         |

Este pipeline cobre a **camada Bronze**, onde os dados sÃ£o extraÃ­dos e armazenados no formato bruto (ex: CSV ou Parquet).

ðŸ”¹ 1. **Criar o Azure Data Factory (ADF)**

1. No portal Azure, vÃ¡ em **"Create a resource" > Data Factory**.
2. Preencha os campos: nome, subscription, resource group, regiÃ£o, etc.
3. Crie e aguarde a implantaÃ§Ã£o.

ðŸ”¹ 2. **Instalar e configurar o Self-hosted Integration Runtime (SHIR)**

1. No ADF, vÃ¡ atÃ© **Manage > Integration Runtimes > + New**.
2. Selecione **Self-hosted** e siga o assistente.
3. Baixe o instalador e instale em uma mÃ¡quina com acesso ao banco SQL.
4. Copie e cole a **chave de autenticaÃ§Ã£o** gerada no ADF durante a instalaÃ§Ã£o.
5. Finalize a instalaÃ§Ã£o e teste a conectividade.

ðŸ”¹ 3. **Criar o Linked Service para o SQL Server on-premises**

1. Em **Manage > Linked services > + New**.
2. Selecione **SQL Server**.
3. Configure os seguintes campos:

   * Nome do servidor: `servidor_sql_local`
   * Nome do banco: `nome_do_banco`
   * AutenticaÃ§Ã£o: SQL ou Windows
   * IR: selecione o **Self-hosted Integration Runtime**
4. Clique em **Test Connection** e depois **Create**.

### ðŸ”¹ 4. **Criar o Linked Service para o Azure Blob Storage**

1. Em **Manage > Linked services > + New**.
2. Selecione **Azure Blob Storage**.
3. Configure:

   * Nome da conta de armazenamento
   * Tipo de autenticaÃ§Ã£o: Chave de conta ou Managed Identity
4. Teste a conexÃ£o e salve.

ðŸ”¹ 5. **Criar o Dataset de origem (SQL Server)**

1. VÃ¡ para **Author > Datasets > + New Dataset**.
2. Selecione **SQL Server**.
3. Vincule ao linked service criado no passo 3.
4. Escolha a tabela `tabela_teste` ou defina uma query SQL.
5. Nomeie como `ds_sql_tabela_teste`.

### ðŸ”¹ 6. **Criar o Dataset de destino (Blob Storage)**

1. VÃ¡ para **Author > Datasets > + New Dataset**.
2. Selecione **Azure Blob Storage** > formato: **DelimitedText (CSV)** ou **Parquet**.
3. Vincule ao linked service do passo 4.
4. Defina:

   * Caminho: `bronze/sql/tabela_teste/yyyy=2025/mm=05/dd=16/`
   * Nome do arquivo: `tabela_teste.csv`
5. Nomeie como `ds_blob_tabela_teste_bronze`.

ðŸ”¹ 7. **Criar o Pipeline de CÃ³pia**

1. VÃ¡ para **Author > Pipelines > + New pipeline**.
2. Nomeie como `pl_copy_sql_to_blob_bronze`.
3. Arraste a atividade **Copy data** para a tela.
4. Na aba **Source**:

   * Dataset: `ds_sql_tabela_teste`
5. Na aba **Sink**:

   * Dataset: `ds_blob_tabela_teste_bronze`
   * (Opcional) Configure opÃ§Ãµes como compressÃ£o ou particionamento.
6. Salve e **Publish All**.

ðŸ”¹ 8. **Executar o Pipeline**

1. No topo da tela, clique em **Trigger > Trigger Now**.
2. Monitore em **Monitor** para ver status e logs.
3. Acesse o Blob Storage e verifique se o arquivo foi criado corretamente no container `bronze`.

# Explorando databricks

## ðŸš€ **1. Acessar a plataforma gratuita do Databricks**

1. VÃ¡ para: [https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)
2. Clique em **"Sign in"** ou **"Sign up"**:

   * Caso nÃ£o tenha conta, clique em **"Sign up"**, informe seus dados (email), e ative a conta pelo link enviado.

3. ApÃ³s login, vocÃª verÃ¡ o **Databricks Workspace** com menus laterais como **Workspace**, **Clusters**, **Jobs**, etc.

---

## âš™ï¸ **2. Criar um cluster (modo single-node)**

> Um cluster Ã© necessÃ¡rio para executar notebooks e processar dados.

1. No menu lateral esquerdo, clique em **Compute** (ou â€œClustersâ€).
2. Clique em **Create Cluster**.
3. Preencha os seguintes campos:

   * **Cluster name:** `single_node_cluster`
   * **Cluster mode:** Selecione **Single Node**
   * **Databricks Runtime Version:** use a versÃ£o mais recente (ex: `11.3 LTS (Scala 2.12, Spark 3.3.0)`).
   * **Node Type:** mantenha o padrÃ£o (ex: `Standard_DS3_v2`)
   * **Worker type:** nÃ£o aplicÃ¡vel, pois Ã© single node.
4. Clique em **Create Cluster**.
5. Aguarde o status mudar para **Running** (leva \~2 minutos).

## ðŸ“’ **3. Criar um notebook e conectar ao cluster**

1. No menu lateral, vÃ¡ em **Workspace > Users > [seu\_email@databricks.com](mailto:seu_email@databricks.com)**.
2. Clique em **Create > Notebook**.
3. Preencha:

   * **Name:** `exemplo_pandas_csv`
   * **Default Language:** Python
   * **Cluster:** selecione `single_node_cluster`
4. Clique em **Create**.

## ðŸ“¥ **4. Ler arquivo CSV de URL externa com Pandas**

### ðŸ“Ž Fonte dos dados:

[https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/products.csv](https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/products.csv)

### âœ… CÃ³digo para leitura e agregaÃ§Ã£o:

```python
# 1. Importar bibliotecas
import pandas as pd

# 2. Ler o CSV da URL
url = "https://raw.githubusercontent.com/MicrosoftLearning/mslearn-databricks/main/data/products.csv"
df = pd.read_csv(url)

# 3. Visualizar as 5 primeiras linhas
display(df.head())

# 4. Agregar por ProductName e Category
agg_df = df.groupby(['ProductName', 'Category']).size().reset_index(name='Total')

# 5. Exibir resultado
display(agg_df)
```

## ðŸ§ª Resultado

VocÃª verÃ¡ uma tabela com a contagem de produtos agrupados por `ProductName` e `Category`.

![image](https://github.com/user-attachments/assets/5672c989-2f0c-4394-bc8f-1ad8c5b2308c)


# Azure Data Fabric & Azure DevOps

# Azure Data Fabric

## ðŸ§© **1. O que Ã© o Microsoft Fabric e como utilizÃ¡-lo**

O **Microsoft Fabric** Ã© uma plataforma unificada de dados como serviÃ§o (Data as a Service - DaaS) da Microsoft que integra **engenharia de dados, ciÃªncia de dados, anÃ¡lise em tempo real, Power BI e governanÃ§a** em uma Ãºnica interface SaaS. Ele Ã© construÃ­do sobre o Power BI e o Azure Synapse, e visa simplificar arquiteturas modernas de dados.

### âœ… **CriaÃ§Ã£o de um workspace**

1. Acesse o portal: [https://app.fabric.microsoft.com/](https://app.fabric.microsoft.com/)
2. FaÃ§a login com uma conta corporativa do Microsoft 365.
3. Crie ou acesse um **workspace** (Ã¡rea de trabalho).
4. Dentro do workspace, vocÃª pode criar diferentes tipos de itens:

   * **Lakehouse** (para armazenar arquivos Parquet/Delta)
   * **Data Warehouse** (para modelo relacional)
   * **Notebook** (para cÃ³digo PySpark)
   * **Pipeline** (semelhante ao Azure Data Factory)
   * **Dataflow Gen2** (ETL visual)
   * **Reports** (com Power BI)
   * **SQL Query** (para consultas)

### ðŸ” **SQL Analytics e Queries**

* VocÃª pode usar a interface de **SQL Query Editor** dentro do **Data Warehouse** ou **Lakehouse**.
* SQL Queries no Fabric permitem consultas sobre:

  * Arquivos Delta armazenados no Lakehouse.
  * Tabelas de data warehouse com recursos como indexing e performance tuning automÃ¡ticos.
* TambÃ©m Ã© possÃ­vel criar **Views, Stored Procedures e Functions** diretamente no editor.

### ðŸ“ˆ **Analytics e ExperiÃªncia Unificada**

* O Microsoft Fabric combina a experiÃªncia de Power BI com Lakehouse e Warehouse:

  * **Real-time Analytics**: com suporte a eventos e dados em tempo real.
  * **OneLake**: camada de armazenamento unificada para todos os artefatos (parquet, delta, warehouse etc).
  * **Auto Discovery**: recursos de descoberta automÃ¡tica de dados nos workspaces.

### ðŸŽ“ **Learning Center (Centro de Aprendizado)**

* DisponÃ­vel no menu lateral esquerdo, o **Learning Center** oferece:

  * **Tutoriais guiados** sobre como usar Lakehouse, Notebooks, Pipelines e SQL queries.
  * **LaboratÃ³rios interativos** com datasets reais.
  * Acesso a **documentaÃ§Ã£o oficial, vÃ­deos e exemplos prÃ¡ticos**.

## ðŸ“Š **2. Monitoramento de recursos, custos e uso da calculadora**

O Microsoft Fabric adota um modelo baseado em **Capacidade de ComputaÃ§Ã£o (Capacity Units - CUs)**, com controle detalhado de uso e custo.

### ðŸ”Ž **Monitoramento de Recursos**

* Acesse via portal do Microsoft Fabric ou Power BI Admin Center:

  * **Usage Metrics**: horas de uso de capacidade, jobs agendados, pipelines, leitura de dados etc.
  * **Monitor Hub**: permite rastrear a execuÃ§Ã£o de notebooks, pipelines e refresh de datasets.

### ðŸ’¸ **Monitoramento de Custos**

* O controle de custo pode ser feito com:

  * **Azure Cost Management** (se o Fabric estiver vinculado a uma assinatura Azure).
  * **Capacity Metrics App**: app oficial que monitora uso da capacidade e projeta o custo.
  * **Power BI Admin Portal**: mostra utilizaÃ§Ã£o por workspace, usuÃ¡rio e tipo de atividade.

### ðŸ“ **Calculadora de PreÃ§o**

* Use a calculadora oficial do Azure para estimar custos:

  * [https://azure.microsoft.com/en-us/pricing/calculator/](https://azure.microsoft.com/en-us/pricing/calculator/)
* Na calculadora:

  1. Selecione â€œMicrosoft Fabricâ€ ou "Power BI Premium Capacity".
  2. Configure:

     * Tipo de capacidade (F SKU, P SKU etc.)
     * NÃºmero de usuÃ¡rios e frequÃªncia de uso.
     * Necessidade de warehouse, pipelines, notebooks.
  3. Visualize o custo estimado mensal.

# Azure DevOps

## ðŸ”§ 1) Componentes do Azure DevOps

O Azure DevOps Ã© um conjunto de ferramentas integradas para planejar, desenvolver, testar e entregar software com mais agilidade e controle.

### âœ… **Azure Boards**

* Ferramenta de gerenciamento Ã¡gil de projetos.
* Permite organizar **work items**, **user stories**, **bugs**, **sprints** e **backlogs**.
* Suporta **Kanban**, **Scrum** e personalizaÃ§Ã£o de workflows.

### ðŸ”„ **Azure Pipelines**

* Sistema de CI/CD para compilar, testar e implantar aplicaÃ§Ãµes.
* Suporta mÃºltiplas linguagens (Python, .NET, Node.js, Java).
* Pode executar em agentes hospedados (Microsoft) ou self-hosted.
* Suporte a pipelines clÃ¡ssicos e YAML-as-code.
* IntegraÃ§Ã£o com Azure, Kubernetes, GitHub e outros.

### ðŸ” **Azure Repos**

* Sistema de versionamento baseado em Git.
* RepositÃ³rios privados ilimitados.
* Suporte a **branches**, **pull requests**, **code reviews** e **merge policies**.

### ðŸ”’ **Security & Policies**

* Gerenciamento granular de permissÃµes por:

  * OrganizaÃ§Ã£o
  * Projeto
  * Pipeline
  * Branch
* **Branch Policies**:

  * Pull Request obrigatÃ³ria
  * ValidaÃ§Ã£o de build
  * AprovaÃ§Ã£o de revisores
  * Controle de quem pode escrever no `main`

### ðŸ“‹ **Test Plans**

* Ferramenta para gerenciamento de testes manuais e automatizados.
* Suporta **casos de teste, testes exploratÃ³rios e execuÃ§Ã£o em mÃºltiplos ambientes**.
* Integra-se com pipelines e dashboards para rastreabilidade.

### ðŸ“¦ **Artifacts**

* Sistema de gerenciamento de pacotes (NuGet, npm, Python, Maven).
* Permite armazenar e compartilhar bibliotecas internas.
* Ideal para projetos com mÃºltiplos serviÃ§os ou times.

## ðŸ§© 2) ExtensÃµes importantes

### ðŸ““ **Jupyter Notebooks**

* ExtensÃ£o que permite rodar, versionar e revisar **notebooks (.ipynb)** diretamente no Azure DevOps.
* Ãštil para projetos de ciÃªncia de dados e anÃ¡lises exploratÃ³rias.
* Suporte a diffs entre versÃµes de notebook.

### ðŸŒ **Terraform**

* ExtensÃ£o para rodar scripts de **Infraestrutura como CÃ³digo (IaC)**.
* Suporta automaÃ§Ã£o de provisionamento de recursos Azure.
* Comandos disponÃ­veis via pipeline:

  * `terraform init`
  * `terraform plan`
  * `terraform apply`

---

## ðŸ—ï¸ 3) OrganizaÃ§Ãµes e Projetos

### ðŸ¢ **OrganizaÃ§Ãµes**

* Unidade raiz do Azure DevOps.
* Ex: `https://dev.azure.com/minhaempresa/`
* Uma organizaÃ§Ã£o pode conter mÃºltiplos projetos.
* Controle de billing, polÃ­ticas de seguranÃ§a e extensÃµes.

### ðŸ“ **Projetos**

* Subunidade para organizar os artefatos de um time/produto.
* Cada projeto tem:

  * Repos prÃ³prios
  * Pipelines
  * Boards
  * Test Plans
  * Artifacts

## ðŸš€ 4) Configurando Azure DevOps para um Azure Data Factory chamado **â€œestudoâ€**

### Etapas para configurar CI/CD para o ADF:

### **1. Exportar o cÃ³digo JSON do Data Factory**

* Acesse o portal do Azure > Data Factory > "Author".
* Use o botÃ£o **"Publish"** para consolidar alteraÃ§Ãµes.
* VÃ¡ em **Manage > Source Control** e conecte com o Azure Repos.
* O cÃ³digo (pipelines, datasets, linked services) serÃ¡ exportado em formato JSON para o repositÃ³rio.

### **2. Estrutura do RepositÃ³rio**

```bash
/adf
  â”œâ”€â”€ datasets/
  â”œâ”€â”€ linkedServices/
  â”œâ”€â”€ pipelines/
  â””â”€â”€ arm-template/
        â”œâ”€â”€ template.json
        â””â”€â”€ parameters.json
```

### **3. Criar um pipeline YAML de deploy**

Arquivo: `.azure-pipelines/deploy-adf.yml`

```yaml
trigger:
  branches:
    include:
      - main

pool:
  vmImage: 'ubuntu-latest'

variables:
  azureSubscription: 'conexao-devops-azure'
  resourceGroupName: 'rg-estudo'
  dataFactoryName: 'adf-estudo'

steps:
- task: AzureResourceManagerTemplateDeployment@3
  inputs:
    deploymentScope: 'Resource Group'
    azureResourceManagerConnection: '$(azureSubscription)'
    subscriptionId: '<ID da SubscriÃ§Ã£o>'
    action: 'Create Or Update Resource Group'
    resourceGroupName: '$(resourceGroupName)'
    location: 'Brazil South'
    templateLocation: 'Linked artifact'
    csmFile: 'adf/arm-template/template.json'
    csmParametersFile: 'adf/arm-template/parameters.json'
    overrideParameters: '-factoryName $(dataFactoryName)'
    deploymentMode: 'Incremental'
```

### **4. Criar um Azure Pipeline com base no YAML**

* Acesse o projeto no Azure DevOps > Pipelines.
* Clique em **â€œNew Pipelineâ€** > escolha o repositÃ³rio > YAML > aponte para `deploy-adf.yml`.

### **5. Adicionar polÃ­ticas e seguranÃ§a**

* VÃ¡ em **Project Settings > Repositories > Branches > main**
* Adicione:

  * PolÃ­ticas de Pull Request
  * Build obrigatÃ³rio antes do merge
  * Revisores obrigatÃ³rios
* Em **Permissions**, controle quem pode publicar e deletar pipelines.
