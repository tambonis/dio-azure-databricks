# Aprendizados do Projeto Azure Databricks

## Este projeto apresenta uma visÃ£o prÃ¡tica do ambiente Azure utilizando uma conta gratuita de estudante. Abaixo estÃ£o os principais tÃ³picos abordados:

ðŸ”¹ CriaÃ§Ã£o de Assinatura Azure: Uso de conta gratuita para ativar e configurar assinatura de estudante no portal Azure.

ðŸ”¹ CriaÃ§Ã£o de Grupos de Recursos: OrganizaÃ§Ã£o lÃ³gica dos recursos do Azure para facilitar gerenciamento e controle.

ðŸ”¹ Boas PrÃ¡ticas de Nomenclatura: AplicaÃ§Ã£o de padrÃµes para nomes de recursos visando padronizaÃ§Ã£o e legibilidade.

ðŸ”¹ ConfiguraÃ§Ã£o do Azure Data Factory: PreparaÃ§Ã£o do ambiente para orquestraÃ§Ã£o de dados e integraÃ§Ã£o com outros serviÃ§os.

ðŸ”¹ Monitoramento de Uso e Custos:

ConfiguraÃ§Ã£o de dashboards personalizados;

Uso de mÃ©tricas de consumo e alertas de custo para controle financeiro.

ðŸ”¹ VisualizaÃ§Ã£o dos Dados de Consumo: InterpretaÃ§Ã£o e anÃ¡lise dos gastos no portal Azure.

ðŸ”¹ Infraestrutura como CÃ³digo:

CriaÃ§Ã£o de templates com ARM Templates;

ImplantaÃ§Ã£o automatizada de recursos.

ðŸ”¹ AutomaÃ§Ã£o com Azure Cloud Shell:

UtilizaÃ§Ã£o de comandos via terminal;

Scripts de automaÃ§Ã£o para criaÃ§Ã£o e gerenciamento de recursos.

# Lab de copiar uma tabela de teste de um SQL Server on-premises self-hosted para o Azure Blob Storage, usando o Azure Data Factory (ADF) por meio de um pipeline, - 30/05/2025

![image](https://github.com/user-attachments/assets/6ca04a7b-6390-4fdc-8270-03522747d636)

Conceitos:

* **Linked Service**
* **Dataset**
* **Pipeline**
* **Self-hosted Integration Runtime**
* **Medallion Architecture** (nÃ­vel Bronze)

## âœ… **Objetivo:**

Mover os dados da tabela `tabela_teste` do SQL Server local para um container no Azure Blob Storage, representando a **camada Bronze** da Medallion Architecture.

## ðŸ§° **Tecnologias envolvidas**

* SQL Server (on-premises)
* Azure Blob Storage
* Azure Data Factory (ADF)
* Self-hosted Integration Runtime (SHIR)

## ðŸ§± Medallion Architecture

| Camada     | DescriÃ§Ã£o                                    |
| ---------- | -------------------------------------------- |
| **Bronze** | Dados brutos, copiados diretamente da origem |
| Silver     | Dados limpos e transformados                 |
| Gold       | Dados prontos para consumo analÃ­tico         |

Este pipeline cobre a **camada Bronze**, onde os dados sÃ£o extraÃ­dos e armazenados no formato bruto (ex: CSV ou Parquet).

ðŸ”¹ 1. **Criar o Azure Data Factory (ADF)**

1. No portal Azure, vÃ¡ em **"Create a resource" > Data Factory**.
2. Preencha os campos: nome, subscription, resource group, regiÃ£o, etc.
3. Crie e aguarde a implantaÃ§Ã£o.

ðŸ”¹ 2. **Instalar e configurar o Self-hosted Integration Runtime (SHIR)**

1. No ADF, vÃ¡ atÃ© **Manage > Integration Runtimes > + New**.
2. Selecione **Self-hosted** e siga o assistente.
3. Baixe o instalador e instale em uma mÃ¡quina com acesso ao banco SQL.
4. Copie e cole a **chave de autenticaÃ§Ã£o** gerada no ADF durante a instalaÃ§Ã£o.
5. Finalize a instalaÃ§Ã£o e teste a conectividade.

ðŸ”¹ 3. **Criar o Linked Service para o SQL Server on-premises**

1. Em **Manage > Linked services > + New**.
2. Selecione **SQL Server**.
3. Configure os seguintes campos:

   * Nome do servidor: `servidor_sql_local`
   * Nome do banco: `nome_do_banco`
   * AutenticaÃ§Ã£o: SQL ou Windows
   * IR: selecione o **Self-hosted Integration Runtime**
4. Clique em **Test Connection** e depois **Create**.

### ðŸ”¹ 4. **Criar o Linked Service para o Azure Blob Storage**

1. Em **Manage > Linked services > + New**.
2. Selecione **Azure Blob Storage**.
3. Configure:

   * Nome da conta de armazenamento
   * Tipo de autenticaÃ§Ã£o: Chave de conta ou Managed Identity
4. Teste a conexÃ£o e salve.

ðŸ”¹ 5. **Criar o Dataset de origem (SQL Server)**

1. VÃ¡ para **Author > Datasets > + New Dataset**.
2. Selecione **SQL Server**.
3. Vincule ao linked service criado no passo 3.
4. Escolha a tabela `tabela_teste` ou defina uma query SQL.
5. Nomeie como `ds_sql_tabela_teste`.

### ðŸ”¹ 6. **Criar o Dataset de destino (Blob Storage)**

1. VÃ¡ para **Author > Datasets > + New Dataset**.
2. Selecione **Azure Blob Storage** > formato: **DelimitedText (CSV)** ou **Parquet**.
3. Vincule ao linked service do passo 4.
4. Defina:

   * Caminho: `bronze/sql/tabela_teste/yyyy=2025/mm=05/dd=16/`
   * Nome do arquivo: `tabela_teste.csv`
5. Nomeie como `ds_blob_tabela_teste_bronze`.

ðŸ”¹ 7. **Criar o Pipeline de CÃ³pia**

1. VÃ¡ para **Author > Pipelines > + New pipeline**.
2. Nomeie como `pl_copy_sql_to_blob_bronze`.
3. Arraste a atividade **Copy data** para a tela.
4. Na aba **Source**:

   * Dataset: `ds_sql_tabela_teste`
5. Na aba **Sink**:

   * Dataset: `ds_blob_tabela_teste_bronze`
   * (Opcional) Configure opÃ§Ãµes como compressÃ£o ou particionamento.
6. Salve e **Publish All**.

ðŸ”¹ 8. **Executar o Pipeline**

1. No topo da tela, clique em **Trigger > Trigger Now**.
2. Monitore em **Monitor** para ver status e logs.
3. Acesse o Blob Storage e verifique se o arquivo foi criado corretamente no container `bronze`.
